{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2338e20-1aef-4ac0-9ff9-d98488297ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da4d02-2d15-415d-adff-bca3f2d50739",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/iris.data', header=None)\n",
    "df.columns = ['sepal_length','sepal_width','petal_length','petal_width','class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bf9e0b-1b0a-49ad-8747-572f6c17402a",
   "metadata": {},
   "source": [
    "In this notebook you'll be introduced to the main idea and workflow of *modeling* data. In this context, a *model* is an algorithm that takes some input `X` and produces some output `y`. Typically, `y` is some information about the data that we want to predict based on the observed `X`. \n",
    "\n",
    "For example, in our Iris dataset, we have *labels* for the species of all 150 flowers (e.g., a botanist looked at each flower and identified its species). Using this dataset, we can *train* a model that predicts the species of flower based on its measurements. Then, if we obtain the measurements of a new flower, we can use our model to predict its species (without showing a photo of the flower to a botanist for identification).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a60881e-76e1-4e74-9cd6-4ab85bdee475",
   "metadata": {},
   "source": [
    "To start, we can split our dataset up into `X` and `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e514d7c0-917e-4903-91e6-4ce7665a52c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['sepal_length','sepal_width','petal_length','petal_width']]\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a42885-d5d2-4e2e-b72e-869d63322982",
   "metadata": {},
   "source": [
    "We will start with a relatively simple type of model called **logistic regression**. A brief review of logistic regression can be found here (todo). Don't worry about understanding the technical details right now, we'll first focus on the high-level process of modeling data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb41fc2a-ca4f-492e-bf6a-2e7e0affe01d",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56ccc2b-2e05-405c-8eee-762e21ad65e6",
   "metadata": {},
   "source": [
    "[Scikit-learn](https://scikit-learn.org/stable/index.html) is a Python library for training and evaluating machine learning models. In this section, we go over the fundamental steps of building a model in scikit-learn. First, we will import the specific model that we need (`LogisticRegression`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11872ca4-a97c-4777-a010-70afaf5c12dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782e9753-dbe0-468d-99d4-8867dc4aae63",
   "metadata": {},
   "source": [
    "We then need to create an instance of the logistic regression model, which can be done by calling `LogisticRegression()`. If we want to specify any particular model settings, we would pass them to the function here (we'll look at this in detail later in this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d738fdbe-3515-406b-92c4-310f2a932d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7732b68a-8750-4077-aec6-1c398e839512",
   "metadata": {},
   "source": [
    "Then, we **train** the model, attempting to teach it to predict `y` from `X`. To do this, we call the `fit` method on the model we created, giving it our `X` and `y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e83ef7b-cc1a-46e6-8a46-150cdcd0c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427b09ee-bfbf-4d35-8105-328cdabcc67f",
   "metadata": {},
   "source": [
    "Now that it has been trained, we can use the model to predict a flower's species given its measurements. For example, the model predicts that a flower with a sepal length, sepal width, petal length, and petal width of 5.0, 4.7, 1.3, and 0.2, respectively, is an Iris setosa flower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c630ad8-e3e3-4bc9-aecf-957a56f44509",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = [5.0,4.7,1.3,0.2] \n",
    "X_new = np.array(X_new).reshape(1, -1) # use numpy to put the new row in the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761d02b5-4970-4587-bbaf-c17c0141c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106c0d52-ea44-4a0c-bfb0-e8fa57350b69",
   "metadata": {},
   "source": [
    "Now that we have our model, it's important to **evaluate** it, or check the quality of its predictions. There are many different model evaluation strategies. For a simple example, we can give the model our data `X` and observe the predictions it makes. These predictions are denoted `Å·`, pronounced \"y hat.\" We can then compare these predictions to the true `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca4fead2-5e48-4122-99ca-ce89d6632ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = lr.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0fcb6e-7ae8-4299-8e74-cf15925388ff",
   "metadata": {},
   "source": [
    "For example, we can check how often the predicted species is equal to the true species (i.e., the *accuracy* of the model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf1efb5-336b-4540-85d9-429a2bfd99bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_hat==y)/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebe7a47-f80f-4ed8-8509-2faadb32673a",
   "metadata": {},
   "source": [
    "Scikit-learn has a built-in method, `score`, to do the same thing (generate predictions from `X` and check accuracy against the true `y`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67af999f-1f30-4f1a-858a-0304ba369287",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45ea3d4-0a29-4cb7-bcc5-79b10b7b78d7",
   "metadata": {},
   "source": [
    "# Notes: Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0552ae-aed2-41a5-b9e6-f360b1b102e4",
   "metadata": {},
   "source": [
    "However, in practice, we don't *evaluate* the model using the same data it was trained on. The reason for this is that we want to quantify how well the model will perform on new data it hasn't seen before. Otherwise, the model could end up \"memorizing\" the training data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f3e343-901a-4fd4-9c3f-ccc39e8b81ea",
   "metadata": {},
   "source": [
    "This is related to the problem of **overfitting**. A common issue with machine learning models is that they can become too specific to the training data, or memorize the training data, and not be well-equipped to generalize to new data. To be more concrete, consider the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ae0281-9a45-40bd-a25c-264f4da31aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec74025a-52ae-4072-b520-0bb4b41af2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_colors = {'Iris-setosa': 'red', 'Iris-versicolor': 'green', 'Iris-virginica': 'blue'}\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "for iris_class in ['Iris-versicolor','Iris-virginica']:\n",
    "    class_data = df[df['class'] == iris_class]\n",
    "    ax.scatter(class_data['sepal_width'], class_data['petal_width'],\n",
    "               color=class_colors[iris_class], label=iris_class) \n",
    "\n",
    "ax.set_xlabel('Sepal Width'); ax.set_ylabel('Petal Width')\n",
    "ax.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4085950b-af10-4984-918a-963478b90e95",
   "metadata": {},
   "source": [
    "Given just the sepal and petal widths of a flower, if we wanted to make a simple \"model\" to predict whether it's a versicolor or virginica flower, we could, for instance, decide based on whether the point falls above or below some dividing line, for example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b8f1ca-5504-4898-88f2-9cff5b93102c",
   "metadata": {},
   "source": [
    "![balance](https://www.markellekelly.com/assets/balance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828ad120-48d6-4c8b-969e-b6b54ba3b6f8",
   "metadata": {},
   "source": [
    "With this approach, a few flowers from both species would be labeled incorrectly, but the line seems like an appropriate summary of the pattern we see. We can imagine that if we added some new flowers to this graph, the line would mostly correctly classify them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607675c4-0a6a-411d-ab8c-0c4d2c1136ae",
   "metadata": {},
   "source": [
    "However, if we wanted to create a model with perfect accuracy on these points, we could draw a line like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f1b602-e326-4253-b409-872741f538a5",
   "metadata": {},
   "source": [
    "![overfit](https://www.markellekelly.com/assets/overfit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429b26cf-5ac3-4fc1-957e-27d934e30334",
   "metadata": {},
   "source": [
    "Using this line to divide the data, all the points in our dataset would be correctly classified. However, it seems unlikely that, if we observed 100 new flowers, that they would all be correctly classified by this line. Thus, this curvy line *overfits* the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b07686-a7f8-41f7-b09e-3e010d5a3ddd",
   "metadata": {},
   "source": [
    "As we've shown here, achieving very high accuracy on the data you already have may not translate to optimal performance on new data. So how can we evaluate our model without waiting for new data to be collected?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219a22f2-1385-454a-97ed-db147de7d12d",
   "metadata": {},
   "source": [
    "# Model Evaluation: Train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd167993-c380-4f20-863a-4f0d0146f22f",
   "metadata": {},
   "source": [
    "In general, we evaluate models using a **train/test split**. This means that, before we perform any modeling on the data, we randomly select some observations to *hold out*. In other words, we split up the dataset, using part of it to train the model while saving some (unseen to the model) data for model evaluation. Scikit-learn has a function [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split up the data for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668ae5a5-897a-4b8a-b2ce-d9d6600c2eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23fe3d7-585a-434e-8e5d-c0c13dab7212",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca0f8dd-d524-43ea-9e64-7be8101feadc",
   "metadata": {},
   "source": [
    "By default, `train_test_split` randomly chooses 25% of the data to save for testing. This testing data is stored in `X_test` and `y_test`, while the subset we'll use for training is `X_train` and `y_train`. (Side note: although these points are randomly chosen, we set the `random_state` argument, which ensures that our results are the same every time). Of our 150 data points, we have 112 points to train the model and 38 that we'll save to test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1805e8b-b9cc-428a-afc9-def6aac309ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a66a9-37d9-4965-897b-15e676b49ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd9a1f9-d0be-4afc-a723-895d3b141e04",
   "metadata": {},
   "source": [
    "Let's re-run our logistic regression using this train/test paradigm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b97c8a-19f0-4ef3-a790-9f54f05d63fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='liblinear', random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e23b75-ffb6-4885-8e95-c87c2daa23f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train) # fit, or train, the model on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bdd477-f3ef-40b6-b244-d0b195f0b347",
   "metadata": {},
   "source": [
    "Our accuracy on the training dataset is relatively high at about 94.6%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88cd6b9-f144-4403-ac75-bd8a0774e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2d41e5-fbc8-46e3-8e8b-38712f3be7ee",
   "metadata": {},
   "source": [
    "However, our *test accuracy* is not as high at about 86.8%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3219162-7973-4e39-a81d-ca7839279b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_test, y_test) # evaluate, or test, the model on the testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b6c592-6ba8-40b9-bc76-b8375cee3d0d",
   "metadata": {},
   "source": [
    "It seems like this logistic regression model is not generalizing to \"new\" flowers as well as it could be. Let's bring out the big guns!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcffe2be-a9b6-4614-9802-841908a4927f",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe10de-f5d3-4f2c-a5ef-54a05acbe0bc",
   "metadata": {},
   "source": [
    "**Neural networks** are the highest-performing, most-discussed machine learning models used today. The \"AI\"s you hear about on the news (e.g., [DALL-E](https://www.wired.com/story/dalle-ai-meme-machine/) or [ChatGPT](https://openai.com/chatgpt)), are (very big and complex) neural networks. We won't get into the technical details of how neural networks work right now, but check out (todo) for the main ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7a3d6c-24ac-4ebb-8113-04e2d34c1f26",
   "metadata": {},
   "source": [
    "We can build a simple neural network in scikit-learn by creating an `MLPClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae2aed5-f7d7-4fc6-81da-8c242b7c5e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2931ab0e-70f4-47cd-8acd-0d258ea146d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=1000, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0184b6bb-21b0-4071-a739-c0302b53c315",
   "metadata": {},
   "source": [
    "Using the same `fit()` method we used for the logistic regression, train the `mlp` on our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbdcf57-e2be-4ed0-aa83-510a387c7f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "... # todo: train the neural network on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd599ae-dda6-45a2-b779-c51ab7184d90",
   "metadata": {},
   "source": [
    "Now, compute the accuracy of the model on our testing data (using `score()`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6791329b-e579-4fcf-b05a-6b6066283e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "... # todo: evaluate the neural network's accuracy on the testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576f7a21-addd-4df0-bead-0bd8196852d4",
   "metadata": {},
   "source": [
    "Nice! Based on our accuracy on the testing data, our neural network can correctly classify new data points with over 97% accuracy! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583bc4f7-4c5e-4480-963b-085df13cc3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml] *",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
